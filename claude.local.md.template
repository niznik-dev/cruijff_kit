# claude.local.md.template

Copy this file to `claude.local.md` in the repository root and customize it with your environment-specific settings.

**IMPORTANT**: `claude.local.md` is in `.gitignore` and should never be committed to version control as it may contain personal paths and preferences.

---

## HPC Environment

- **Cluster**: `<cluster_name>` (e.g., Della, Traverse)
- **Documentation**: `<cluster_docs_url>`
- **Username**: `<username>`
- **Group**: `<group>` (e.g., MSALGANIK, CSES)
- **Home directory**: `/home/<username>`
- **Scratch directory**: `/scratch/gpfs/<group>/<username>`
- **Working directory**: `<path_to_cruijff_kit_repo>`

### Shared Resources

- **Models directory**: `/scratch/gpfs/<group>/pretrained-llms`
- **Shared datasets**: `/scratch/gpfs/<group>/shared_data` (if applicable)

## SLURM Defaults

- **Account**: `<your_slurm_account>`
- **Partition**: `<partition>` (leave empty if cluster doesn't use partitions)
- **Constraint**: `<gpu_constraint>` (e.g., `gpu80` for 80GB VRAM GPUs)
- **Default time**: `0:59:00` (under 1 hour to use test queue)
- **Default GPUs**: `1`
- **Default conda environment**: `cruijff` (recommended)

### GPU Configuration

Describe the GPU types available on your cluster so Claude can map model VRAM requirements to the correct SLURM flags.

- **Available GPU types**: `<gpu_type> <vram>GB` (e.g., "A100 80GB", "H100 80GB", "A100 40GB MIG")
- **Full GPU constraint**: `<constraint>` — SLURM constraint for requesting a full dedicated GPU (e.g., `gpu80`)
- **Shared/MIG partition**: `<partition>` — SLURM partition for shared or MIG-partitioned GPUs, if available (e.g., `nomig`). Leave empty if not applicable.
- **MIG notes**: On some clusters, MIG-partitioned GPUs report GPU utilization as `[N/A]` in nvidia-smi. Memory and power metrics remain available.

## Module System

If your cluster uses environment modules:

```bash
module load anaconda3/2025.6
# Add other required modules here
```

If not using modules, remove this section.

## Conda Environment Setup

**Environment name**: `cruijff` (recommended)

**Activation**:
```bash
module load anaconda3/2025.6  # If using modules
conda activate cruijff
```

**Installation**: Create a conda environment and run `make install` (or `make install-dev` for development) from the cruijff_kit repo root. See the Makefile for details.

## GitHub Configuration

- **Username**: `<your_github_username>`
- **cruijff_kit project number**: `3`

**Adding issues to project**:
```bash
gh project item-add 3 --owner niznik-dev --url https://github.com/niznik-dev/cruijff_kit/issues/<issue_number>
```

## Weights & Biases

- **Entity**: `<your_wandb_entity>` (username or team name)
- **Default project**: `<default_wandb_project>`

## Common Paths

### Pretrained Models
- **Location**: `/scratch/gpfs/<group>/pretrained-llms/`
- **Available models**:
  - `Llama-3.2-1B`
  - `Llama-3.2-1B-Instruct`
  - `Llama-3.2-3B-Instruct`
  - `Llama-3.1-8B-Instruct`
  - `Llama-3.3-70B-Instruct`
  - `Qwen2.5-3B`
  - `Qwen2.5-3B-Instruct`
  - (add others as you download them)

### Working Directories
- **Experiments**: `/scratch/gpfs/<group>/<username>/ck-experiments/` — experiment designs and configs
- **Sanity checks**: `/scratch/gpfs/<group>/<username>/ck-sanity-checks/` — workflow validation runs
- **Outputs**: `/scratch/gpfs/<group>/<username>/ck-outputs/` — checkpoints, logs, eval results

### Data
- **Input data**: `/scratch/gpfs/<group>/<username>/data/`
- **Task-specific data**: `<path_to_task_data>`

## Quick Commands

### Fine-tuning (manual)
```bash
# From your run directory (e.g., ck-experiments/<experiment_name>/<run_name>/)
# Using a setup config (preferred):
python <path_to_cruijff_kit>/tools/torchtune/setup_finetune.py \
  --config_file setup_finetune.yaml

# Submit job
sbatch finetune.slurm
```

### Evaluation (manual)
```bash
# First create eval_config.yaml in the eval directory (see inspect_agent.md for schema)
# Then render the SLURM script from the template:
cd /path/to/experiment/<run>/eval
python <path_to_cruijff_kit>/tools/inspect/setup_inspect.py \
  --config eval_config.yaml \
  --model_name Llama-3.2-1B-Instruct \
  --account <slurm_account> \
  --time 0:10:00

# Submit evaluation job
sbatch <task>_epoch<N>.slurm

# View results interactively (on login node)
inspect view --port=$(get_free_port)
```

### Job Management
```bash
# View all your jobs
squeue -u <username>

# View recent job history
sacct -u <username> -S today --format=JobID,JobName,State,ExitCode,Elapsed,MaxRSS,End -X

# Cancel a job
scancel <job_id>

# Cancel all your jobs
scancel -u <username>
```

## Compute Observability

- **jobstats**: Auto-detected at runtime via `check_jobstats_available()`. No configuration needed. When available, provides per-node CPU utilization, memory usage, and resource recommendations. Falls back to seff for CPU data when unavailable.

## Notes

For a filled-in example, see `claude.local.md.della` (configured for the MSALGANIK group on Princeton's Della cluster).

- This file is ignored by git and should remain local
- Update paths and settings as your workflow evolves
- Add experiment notes, common issues, or helpful reminders below
- This file contains environment-specific details only
- Personal communication preferences belong in `~/.claude/CLAUDE.md`
