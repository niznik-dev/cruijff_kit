{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspect-viz Examples for Capitalization Experiments\n",
    "\n",
    "This notebook demonstrates how to use `inspect-viz` pre-built views to visualize\n",
    "evaluation results from cruijff_kit experiments.\n",
    "\n",
    "## Pre-built Views Demonstrated\n",
    "\n",
    "| View | Use Case | Example |\n",
    "|------|----------|----------|\n",
    "| `scores_by_task` | Multiple tasks/conditions | Word length comparison |\n",
    "| `scores_heatmap` | Model × task matrix | Model performance grid |\n",
    "| `scores_radar_by_task` | Multiple metrics | Match vs. includes accuracy |\n",
    "| `scores_by_factor` | Binary factors | With/without prompt |\n",
    "| `scores_by_model` | Cross-model comparison | Llama vs Gemma |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add cruijff_kit to path for imports\n",
    "CRUIJFF_KIT_ROOT = \"/home/st0898/MSALGANIK/st0898/cruijff_kit\"\n",
    "sys.path.insert(0, CRUIJFF_KIT_ROOT)\n",
    "\n",
    "from inspect_viz import Data\n",
    "from inspect_viz.plot import write_html\n",
    "from inspect_viz.view.beta import (\n",
    "    scores_by_task,\n",
    "    scores_heatmap,\n",
    "    scores_radar_by_task,\n",
    "    scores_radar_by_task_df,\n",
    "    scores_by_model,\n",
    "    scores_by_factor,\n",
    ")\n",
    "\n",
    "from tools.inspect.viz_helpers import load_experiment_logs\n",
    "\n",
    "# Output directory for HTML files\n",
    "HTML_OUTPUT_DIR = os.path.join(CRUIJFF_KIT_ROOT, \"viz_examples\", \"html\")\n",
    "os.makedirs(HTML_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Word Length (scores_by_task, scores_heatmap, scores_radar)\n",
    "\n",
    "Compare model performance across different word lengths (tasks).\n",
    "\n",
    "**Variables:**\n",
    "- Model: Llama-3.2-1B vs Llama-3.2-3B\n",
    "- Task: 5-letter words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment data\n",
    "logs_df_wordlen = load_experiment_logs(\n",
    "    experiment_path=\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_wordlen_2026-01-12\",\n",
    "    subdirs=[\"Llama-3.2-1B-Instruct_5L\", \"Llama-3.2-3B-Instruct_5L\"],\n",
    "    log_viewer_url=\"http://localhost:8000/cap_wordlen_logs_viewer/\",\n",
    "    metadata_extractors={\n",
    "        # Extract model size and epoch for cleaner display\n",
    "        \"model\": lambda df: (\n",
    "            df['model'].str.extract(r'Llama-3\\.2-(?P<size>\\d+B)', expand=False)\n",
    "            + '_epoch'\n",
    "            + df['model'].str.extract(r'epoch_(?P<epoch>\\d+)', expand=False)\n",
    "        ),\n",
    "        # Extract word length as task name\n",
    "        \"task_name\": lambda df: df['task_arg_data_path'].str.extract(\n",
    "            r'words_(?P<task_name>\\d+L)', expand=False\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wrap in Data object for inspect-viz\n",
    "evals_wordlen = Data.from_dataframe(logs_df_wordlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scores_by_task: Compare scores across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match accuracy by task\n",
    "scores_by_task_match = scores_by_task(\n",
    "    evals_wordlen,\n",
    "    task_name='task_name',\n",
    "    score_value=\"score_match_accuracy\",\n",
    "    score_stderr=\"score_match_stderr\",\n",
    "    score_label=\"Match Accuracy\",\n",
    "    ci=0.95\n",
    ")\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_by_task_wordlen_match.html\"), scores_by_task_match)\n",
    "\n",
    "# Includes accuracy by task\n",
    "scores_by_task_includes = scores_by_task(\n",
    "    evals_wordlen,\n",
    "    task_name='task_name',\n",
    "    score_value=\"score_includes_accuracy\",\n",
    "    score_stderr=\"score_includes_stderr\",\n",
    "    score_label=\"Includes Accuracy\",\n",
    "    ci=0.95\n",
    ")\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_by_task_wordlen_includes.html\"), scores_by_task_includes)\n",
    "\n",
    "scores_by_task_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scores_heatmap: Model × Task matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match accuracy heatmap\n",
    "heatmap_match = scores_heatmap(\n",
    "    evals_wordlen,\n",
    "    task_name='task_name',\n",
    "    model_name=\"model_display_name\",\n",
    "    model_label=\"Model\",\n",
    "    score_value=\"score_match_accuracy\",\n",
    "    tip=True,\n",
    "    title=\"\"\n",
    ")\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_heatmap_wordlen_match.html\"), heatmap_match)\n",
    "\n",
    "# Includes accuracy heatmap (horizontal orientation)\n",
    "heatmap_includes = scores_heatmap(\n",
    "    evals_wordlen,\n",
    "    task_name='task_name',\n",
    "    model_name=\"model_display_name\",\n",
    "    model_label=\"Model\",\n",
    "    score_value=\"score_includes_accuracy\",\n",
    "    tip=True,\n",
    "    title=\"\",\n",
    "    orientation=\"horizontal\"\n",
    ")\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_heatmap_wordlen_includes.html\"), heatmap_includes)\n",
    "\n",
    "heatmap_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scores_radar_by_task: Multiple metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare radar data with min-max normalization\n",
    "radar_df = scores_radar_by_task_df(\n",
    "    logs_df_wordlen,\n",
    "    invert=[\"score_match_accuracy\", \"score_includes_accuracy\"],\n",
    "    normalization=\"min_max\",\n",
    "    domain=(0, 1)\n",
    ")\n",
    "\n",
    "# Create radar plot showing both scorers\n",
    "radar_plot = scores_radar_by_task(Data.from_dataframe(radar_df))\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_radar_wordlen.html\"), radar_plot)\n",
    "\n",
    "radar_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: Model × System Prompt (scores_by_factor)\n",
    "\n",
    "Compare model performance with and without a system prompt.\n",
    "\n",
    "**Variables:**\n",
    "- Model: Llama-3.2-1B vs Llama-3.2-3B\n",
    "- Factor: with_prompt vs no_prompt (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment data\n",
    "logs_df_prompt = load_experiment_logs(\n",
    "    experiment_path=\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_model_prompt_2025-11-21\",\n",
    "    subdirs=[\n",
    "        \"Llama-3.2-1B-Instruct_no_prompt\",\n",
    "        \"Llama-3.2-3B-Instruct_no_prompt\",\n",
    "        \"Llama-3.2-1B-Instruct_with_prompt\",\n",
    "        \"Llama-3.2-3B-Instruct_with_prompt\"\n",
    "    ],\n",
    "    log_viewer_url=\"http://localhost:8000/cap_prompts_logs_viewer/\",\n",
    "    metadata_extractors={\n",
    "        \"model\": lambda df: df['model_path'].str.extract(\n",
    "            r'(Llama-3\\.2-\\d+B)', expand=False\n",
    "        ),\n",
    "        \"prompt_type\": lambda df: df['task_arg_config_path'].str.extract(\n",
    "            r'Instruct_(?P<prompt_type>with_prompt|no_prompt)', expand=False\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert prompt_type to boolean for scores_by_factor\n",
    "logs_df_prompt['prompt_type'] = logs_df_prompt['prompt_type'] == 'with_prompt'\n",
    "\n",
    "evals_prompt = Data.from_dataframe(logs_df_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scores_by_factor: Binary factor comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match accuracy by factor\n",
    "factor_match = scores_by_factor(\n",
    "    evals_prompt,\n",
    "    factor=\"prompt_type\",\n",
    "    factor_labels=(\"No Prompt\", \"Prompt\"),\n",
    "    score_value=\"score_match_accuracy\",\n",
    "    score_stderr=\"score_match_stderr\",\n",
    "    score_label=\"Match Accuracy\",\n",
    "    model=\"model\",\n",
    "    model_label=\"Model\",\n",
    "    ci=0.95\n",
    ")\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_by_factor_prompt_match.html\"), factor_match)\n",
    "\n",
    "# Includes accuracy by factor\n",
    "factor_includes = scores_by_factor(\n",
    "    evals_prompt,\n",
    "    factor=\"prompt_type\",\n",
    "    factor_labels=(\"No Prompt\", \"Prompt\"),\n",
    "    score_value=\"score_includes_accuracy\",\n",
    "    score_stderr=\"score_includes_stderr\",\n",
    "    score_label=\"Includes Accuracy\",\n",
    "    model=\"model\",\n",
    "    model_label=\"Model\",\n",
    "    ci=0.95\n",
    ")\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_by_factor_prompt_includes.html\"), factor_includes)\n",
    "\n",
    "factor_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: Cross-Organization Model Comparison (scores_by_model)\n",
    "\n",
    "Compare models from different organizations.\n",
    "\n",
    "**Variables:**\n",
    "- Model: Google Gemma-2B vs Meta Llama-3.2-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment data\n",
    "logs_df_crossorg = load_experiment_logs(\n",
    "    experiment_path=\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_cross_org_models_2025-11-21\",\n",
    "    subdirs=[\"Google-Gemma-2B\", \"Meta-Llama-3.2-1B-Instruct\"],\n",
    "    log_viewer_url=\"http://localhost:8000/cap_crossorg_logs_viewer/\",\n",
    "    metadata_extractors={\n",
    "        \"model\": lambda df: df['model_path'].str.extract(\n",
    "            r'(Google-Gemma-2B|Meta-Llama-3\\.2-1B-Instruct)', expand=False\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "evals_crossorg = Data.from_dataframe(logs_df_crossorg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scores_by_model: Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match accuracy by model\n",
    "model_match = scores_by_model(\n",
    "    evals_crossorg,\n",
    "    score_value=\"score_match_accuracy\",\n",
    "    score_stderr=\"score_match_stderr\",\n",
    "    score_label=\"Match Accuracy\",\n",
    "    ci=0.95,\n",
    "    height=200\n",
    ")\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_by_model_crossorg_match.html\"), model_match)\n",
    "\n",
    "# Includes accuracy by model\n",
    "model_includes = scores_by_model(\n",
    "    evals_crossorg,\n",
    "    score_value=\"score_includes_accuracy\",\n",
    "    score_stderr=\"score_includes_stderr\",\n",
    "    score_label=\"Includes Accuracy\",\n",
    "    ci=0.95,\n",
    "    height=200\n",
    ")\n",
    "write_html(os.path.join(HTML_OUTPUT_DIR, \"scores_by_model_crossorg_includes.html\"), model_includes)\n",
    "\n",
    "model_match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
