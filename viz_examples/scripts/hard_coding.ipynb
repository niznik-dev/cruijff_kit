{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0833add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment for anywidget\n",
    "import os\n",
    "os.environ['ANYWIDGET_HUB'] = 'false'\n",
    "\n",
    "# packages and libraries\n",
    "from inspect_ai.log import read_eval_log\n",
    "from inspect_ai.analysis import log_viewer, prepare, evals_df, samples_df, EvalModel, EvalResults, EvalScores, EvalInfo, EvalTask, SampleSummary, model_info, task_info\n",
    "from inspect_viz import Data\n",
    "from inspect_viz.plot import plot\n",
    "from inspect_viz.view.beta import scores_by_task, scores_heatmap, scores_radar_by_task, scores_by_model, scores_radar_by_task_df, scores_by_factor, scores_timeline, scores_by_limit_df, scores_by_limit\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987fdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to prep sample-level data for plotting\n",
    "def samples_df_prep(logs):\n",
    "    logs_df = samples_df(\n",
    "        logs=logs,\n",
    "        columns=(SampleSummary + EvalInfo + EvalModel)\n",
    "    )\n",
    "    return logs_df\n",
    "\n",
    "# helper function to prep eval-level data for plotting\n",
    "def evals_df_prep(logs):\n",
    "    # Read into dataframe\n",
    "    logs_df = evals_df(\n",
    "        logs=logs,\n",
    "        columns=(EvalInfo + EvalTask + EvalModel + EvalResults + EvalScores)\n",
    "    )\n",
    "    return logs_df\n",
    "\n",
    "def load_experiment_logs(experiment_path, subdirs, log_viewer_url, metadata_extractors, samples=False):\n",
    "    \"\"\"\n",
    "    Load evaluation logs from an experiment with multiple subdirectories.\n",
    "      \n",
    "    Parameters:\n",
    "    - experiment_path: Path to main experiment directory\n",
    "    - subdirs: List of subdirectory names containing eval/logs\n",
    "    - log_viewer_url: URL for log viewer (e.g., \"http://localhost:8000/my_logs/\")\n",
    "    - metadata_extractors: Dict mapping column names to lambda functions\n",
    "    E.g., {\"model\": lambda df: df['model_path'].str.extract(r'...', expand=False)}\n",
    "      \n",
    "    Returns: Prepared dataframe ready for visualization\n",
    "    \"\"\"\n",
    "    # Build paths and LOG_DIRS mapping\n",
    "    log_paths = [os.path.join(experiment_path, subdir, \"eval\", \"logs\") for subdir in subdirs]\n",
    "    LOG_DIRS = {path: log_viewer_url for path in log_paths}\n",
    "\n",
    "    # Collect all log files\n",
    "    logs = []\n",
    "    for path in log_paths:\n",
    "        logs.extend([os.path.join(path, f) for f in os.listdir(path)])\n",
    "\n",
    "    # Extract model paths\n",
    "    model_paths = [read_eval_log(log).eval.model_args['model_path'] for log in logs]\n",
    "\n",
    "    if samples:\n",
    "        # Read into sample-level dataframe\n",
    "        logs_df = samples_df_prep(logs)\n",
    "    else:\n",
    "        logs_df = evals_df_prep(logs)\n",
    "\n",
    "    # Add model_path and custom metadata\n",
    "    logs_df['model_path'] = model_paths\n",
    "    for col_name, extractor in metadata_extractors.items():\n",
    "        logs_df[col_name] = extractor(logs_df)\n",
    "\n",
    "    # Prepare with log viewer\n",
    "    \n",
    "    return prepare(logs_df, [log_viewer(\"eval\", LOG_DIRS)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93041b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment 1 (model x wordlength) logs\n",
    "logs_df = load_experiment_logs(\n",
    "    experiment_path=\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_wordlen_2026-01-12\",\n",
    "    subdirs=[\"Llama-3.2-1B-Instruct_5L\", \"Llama-3.2-3B-Instruct_5L\"],\n",
    "    log_viewer_url=\"http://localhost:8000/cap_wordlen_logs_viewer/\",\n",
    "    metadata_extractors={\n",
    "        # new model naming convention automatically identifies 4 different model types\n",
    "        # here we further separate by task and epoch\n",
    "        # cleaned model name: {size}_epoch{epoch} (same information as default, but easier to read)\n",
    "      \"model\": lambda df: df['model'].str.extract(r'Llama-3\\.2-(?P<size>\\d+B)', expand=False) + '_epoch' + df['model'].str.extract(r'epoch_(?P<epoch>\\d+)', expand=False),\n",
    "      # overwrite task_name to reflect word length\n",
    "      \"task_name\": lambda df: df['task_arg_data_path'].str.extract(r'words_(?P<task_name>\\d+L)', expand=False)\n",
    "    },\n",
    "    samples=False\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb87f423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ba92737bb04054aabafc24833a1f8a",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"barY\",\"data\":{\"from\":\"Jon2CgnbyiZksF8qRz534H\",\"filterBy\":\"$selec…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create scores_by_task plot (experiment 1: model x wordlength)\n",
    "evals = Data.from_dataframe(logs_df)\n",
    "\n",
    "# scores_by_task match accuracy plot\n",
    "scores_by_task(\n",
    "    evals,\n",
    "    task_name='task_name',\n",
    "    score_value=\"score_match_accuracy\",\n",
    "    score_stderr=\"score_match_stderr\",\n",
    "    score_label=\"Match Accuracy\",\n",
    "    ci = 0.95\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7c0ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee4a0db1484b07a59334aaafee65c5",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"barY\",\"data\":{\"from\":\"Jon2CgnbyiZksF8qRz534H\",\"filterBy\":\"$selec…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores_by_task includes accuracy plot\n",
    "scores_by_task(\n",
    "    evals,\n",
    "    task_name='task_name',\n",
    "    score_value=\"score_includes_accuracy\",\n",
    "    score_stderr=\"score_includes_stderr\",\n",
    "    score_label=\"Includes Accuracy\",\n",
    "    ci = 0.95\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1wpelavm82u",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b992b5aa483b47ff8375dcf770a7296b",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"text\",\"text\":[\"\"],\"dy\":-20,\"frameAnchor\":\"top\",\"fontSize\":16,\"fa…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create heatmap plot (experiment 1: model x wordlength)\n",
    "# match accuracy heatmap\n",
    "scores_heatmap(\n",
    "    evals,\n",
    "    task_name='task_name',\n",
    "    model_name = \"model_display_name\",\n",
    "    model_label = \"Model\",\n",
    "    score_value=\"score_match_accuracy\",\n",
    "    tip = True,\n",
    "    title = \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0ed4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794f7137c1424970ae8df761f4488483",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"text\",\"text\":[\"\"],\"dy\":-20,\"frameAnchor\":\"top\",\"fontSize\":16,\"fa…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# includes accuracy heatmap\n",
    "scores_heatmap(\n",
    "    evals,\n",
    "    task_name='task_name',\n",
    "    model_name = \"model_display_name\",\n",
    "    model_label = \"Model\",\n",
    "    score_value=\"score_includes_accuracy\",\n",
    "    tip = True,\n",
    "    title = \"\",\n",
    "    orientation = \"horizontal\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42a613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c980f77bd058475a8d871e8468815068",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"line\",\"data\":{\"from\":\"3sFS4ndsgsAjxnuPRm6ibW\",\"filterBy\":\"$selec…"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create scores_radar_by_task_df with min-max normalization\n",
    "evals_scores = scores_radar_by_task_df(\n",
    "    logs_df,\n",
    "    normalization=\"min_max\",\n",
    "    domain=(0, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match accuracy radar plot\n",
    "evals_radar = Data.from_dataframe(evals_scores)\n",
    "scores_radar_by_task(evals_radar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed83013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment 2 (model x system prompt) data\n",
    "logs_df_2 = load_experiment_logs(\n",
    "    experiment_path=\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_model_prompt_2025-11-21\",\n",
    "    subdirs=[\"Llama-3.2-1B-Instruct_no_prompt\", \"Llama-3.2-3B-Instruct_no_prompt\", \"Llama-3.2-1B-Instruct_with_prompt\", \"Llama-3.2-3B-Instruct_with_prompt\"],\n",
    "    log_viewer_url=\"http://localhost:8000/cap_prompts_logs_viewer/\",\n",
    "    metadata_extractors={\n",
    "        \"model\": lambda df: df['model_path'].str.extract(r'(Llama-3\\.2-\\d+B)', expand=False),\n",
    "        \"prompt_type\": lambda df: df['task_arg_config_path'].str.extract(r'Instruct_(?P<prompt_type>with_prompt|no_prompt)', expand=False)\n",
    "    },\n",
    "    samples=False\n",
    "  )\n",
    "\n",
    "# convert prompt_type to boolean (True for with_prompt, False for no_prompt)\n",
    "logs_df_2['prompt_type'] = logs_df_2['prompt_type'] == 'with_prompt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09gsurcpfld4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c6132de23a4b3d8518517e5bc06500",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"frame\",\"anchor\":\"left\",\"insetTop\":5,\"insetBottom\":5},{\"mark\":\"ru…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create scores_by_factor plot (experiment 2: model x system prompt)\n",
    "evals = Data.from_dataframe(logs_df_2)\n",
    "scores_by_factor(evals, \"prompt_type\", (\"No Prompt\", \"Prompt\"))\n",
    "\n",
    "# NOTE: log viewer does not appear (scores_by_factor does not link to logs) - needs investigation\n",
    "# this occurs in the example docs as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cdd3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment 4 (cross-organization model comparison)\n",
    "logs_df_4 = load_experiment_logs(\n",
    "    experiment_path=\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_cross_org_models_2025-11-21\",\n",
    "    subdirs=[\"Google-Gemma-2B\", \"Meta-Llama-3.2-1B-Instruct\"],\n",
    "    log_viewer_url=\"http://localhost:8000/cap_crossorg_logs_viewer/\",\n",
    "    metadata_extractors={\n",
    "        \"model\": lambda df: df['model_path'].str.extract(r'(Google-Gemma-2B|Meta-Llama-3\\.2-1B-Instruct)', expand=False)\n",
    "    },\n",
    "    samples=False\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a1e3a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d112c3cf9c447559c0aeba06352e0c8",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"ruleY\",\"data\":{\"from\":\"HJfDdzcFxf38KGZCVoXRHM\",\"filterBy\":\"$sele…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create scores_by_model plot (experiment 4: cross-organization model comparison)\n",
    "evals_4 = Data.from_dataframe(logs_df_4)\n",
    "scores_by_model(evals_4, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_668824/2859600848.py:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"model\": lambda df: df['model_path'].str.extract(r'(Llama-3\\.2-\\d+B)', expand=False)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\" # load experiment 3 data (model x token limit)\n",
    "logs_df_3 = load_experiment_logs(\n",
    "    experiment_path=\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_token_limit_2026-01-06\",\n",
    "    subdirs=[\"Llama-3.2-1B-Instruct\", \"Llama-3.2-1B-Instruct\"],\n",
    "    log_viewer_url=\"http://localhost:8000/cap_tokenlim_viewer/\",\n",
    "    metadata_extractors={\n",
    "        \"model\": lambda df: df['model_path'].str.extract(r'(Llama-3\\.2-\\d+B)', expand=False)\n",
    "\n",
    "    },\n",
    "    samples=True\n",
    "  )  \"\"\"\n",
    "\n",
    "\n",
    "# Build paths and LOG_DIRS mapping\n",
    "log_paths = [os.path.join(\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_token_limit_2026-01-06\", subdir, \"eval\", \"logs\") for subdir in [\"Llama-3.2-1B-Instruct\", \"Llama-3.2-3B-Instruct\"]]\n",
    "LOG_DIRS = {path: \"http://localhost:8000/cap_tokenlim_viewer/\" for path in log_paths}\n",
    "\n",
    "# Collect all log files\n",
    "logs = []\n",
    "for path in log_paths:\n",
    "    logs.extend([os.path.join(path, f) for f in os.listdir(path)])\n",
    "\n",
    "# Extract model paths and token_limits\n",
    "model_paths = [read_eval_log(log).eval.model_args['model_path'] for log in logs]\n",
    "token_limits = [read_eval_log(log).eval.task_args['max_tokens'] for log in logs]\n",
    "\n",
    "# Read into sample-level dataframe\n",
    "logs_df = samples_df(\n",
    "        logs=logs,\n",
    "        columns=(SampleSummary + EvalInfo + EvalModel)\n",
    "    )\n",
    "\n",
    "# Create mapping dictionaries from eval_id to values\n",
    "eval_ids = logs_df['eval_id'].unique()\n",
    "eval_to_model = dict(zip(eval_ids, model_paths))\n",
    "eval_to_limit = dict(zip(eval_ids, token_limits))\n",
    "\n",
    "# Map values using the dictionaries\n",
    "logs_df['model_path'] = logs_df['eval_id'].map(eval_to_model)\n",
    "logs_df['limit'] = logs_df['eval_id'].map(eval_to_limit)\n",
    "\n",
    "# Ensure limit is numeric\n",
    "#logs_df['limit'] = pd.to_numeric(logs_df['limit'])\n",
    "\n",
    "# Replace \"model\" field with the Llama model that was used\n",
    "logs_df['model'] = logs_df['model_path'].str.extract(r'cap-token-limit-(1B|3B)', expand=False).apply(\n",
    "    lambda x: f'Llama-3.2-{x}-Instruct' if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "# Convert score_match to numeric (1 for correct, 0 for incorrect)\n",
    "logs_df['score_match'] = (logs_df['score_match'] == 'C').astype(int)\n",
    "\n",
    "logs_df = scores_by_limit_df(\n",
    "    logs_df,\n",
    "    score=\"score_match\",\n",
    "    limit = \"limit\"\n",
    ")\n",
    "\n",
    "logs_df_3 = prepare(logs_df, [\n",
    "  model_info(),\n",
    "  log_viewer(\"eval\", LOG_DIRS)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43bc0382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9681d0a565d54fe39ee2a8ac5da68563",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"line\",\"data\":{\"from\":\"3LV3V3EhRLixpeSeELk4ft\",\"filterBy\":\"$selec…"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create scores_by_limit plot\n",
    "evals_3 = Data.from_dataframe(logs_df_3)\n",
    "scores_by_limit(evals_3, limit=\"limit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0549cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# new scores_by_limit plot\n",
    "\n",
    "# Build paths and LOG_DIRS mapping\n",
    "log_paths = [os.path.join(\"/home/st0898/MSALGANIK/st0898/ck-experiments/cap_5L_scores_limit_2026-01-16\", subdir, \"eval\", \"logs\") for subdir in [\"Llama-3.2-1B-Instruct\", \"Llama-3.2-3B-Instruct\"]]\n",
    "LOG_DIRS = {path: \"http://localhost:8000/cap_tokenlim_viewer/\" for path in log_paths}\n",
    "\n",
    "# Collect all log files\n",
    "logs = []\n",
    "for path in log_paths:\n",
    "    logs.extend([os.path.join(path, f) for f in os.listdir(path)])\n",
    "\n",
    "# Extract model paths\n",
    "model_paths = [read_eval_log(log).eval.model_args['model_path'] for log in logs]\n",
    "# token_limits = [read_eval_log(log).eval.task_args['max_tokens'] for log in logs]\n",
    "\n",
    "# Read into sample-level dataframe\n",
    "logs_df = samples_df(\n",
    "        logs=logs,\n",
    "        columns=(SampleSummary + EvalInfo + EvalModel)\n",
    "    )\n",
    "\n",
    "# Create mapping dictionaries from eval_id to values\n",
    "eval_ids = logs_df['eval_id'].unique()\n",
    "eval_to_model = dict(zip(eval_ids, model_paths))\n",
    "\n",
    "# Map values using the dictionaries\n",
    "logs_df['model_path'] = logs_df['eval_id'].map(eval_to_model)\n",
    "# logs_df['limit'] = logs_df['eval_id'].map(eval_to_limit)\n",
    "\n",
    "# Ensure limit is numeric\n",
    "#logs_df['limit'] = pd.to_numeric(logs_df['limit'])\n",
    "\n",
    "# Replace \"model\" field with the Llama model that was used\n",
    "#logs_df['model'] = logs_df['model_path'].str.extract(r'cap-token-limit-(1B|3B)', expand=False).apply(\n",
    " #   lambda x: f'Llama-3.2-{x}-Instruct' if pd.notna(x) else x\n",
    "#)\n",
    "\n",
    "# Convert score_match to numeric (1 for correct, 0 for incorrect)\n",
    "logs_df['score_match'] = (logs_df['score_match'] == 'C').astype(int)\n",
    "\n",
    "logs_df = scores_by_limit_df(\n",
    "    logs_df,\n",
    "    score=\"score_match\"\n",
    ")\n",
    "\n",
    "logs_df_3 = prepare(logs_df, [\n",
    "  model_info(),\n",
    "  log_viewer(\"eval\", LOG_DIRS)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scores_radar_by_metric plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample_heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c3ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8c809ac8db47119a6a8e356f9bdbcb",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Component(spec='{\"hconcat\":[{\"plot\":[{\"mark\":\"ruleY\",\"data\":{\"from\":\"7q9zvVoVrDvUbLijLb7hnN\",\"filterBy\":\"$sele…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# check model name from Mattie's experiment\n",
    "\n",
    "# Build paths and LOG_DIRS mapping\n",
    "log_paths = [os.path.join(\"/home/st0898/MSALGANIK/niznik/ck-sanity-checks/workflow_test_base_2026-01-07\", subdir, \"eval\", \"logs\") for subdir in [\"Llama-3.2-1B-Instruct_base\", \"Llama-3.2-1B-Instruct_rank4\"]]\n",
    "LOG_DIRS = {path: \"http://localhost:8000/test_viewer/\" for path in log_paths}\n",
    "\n",
    "# Collect all log files\n",
    "logs = []\n",
    "for path in log_paths:\n",
    "    logs.extend([os.path.join(path, f) for f in os.listdir(path)])\n",
    "\n",
    "# Read into dataframe\n",
    "logs_df = evals_df(\n",
    "    logs=logs,\n",
    "    columns=(EvalInfo + EvalTask + EvalModel + EvalResults + EvalScores)\n",
    ")\n",
    "\n",
    "evals = Data.from_dataframe(logs_df)\n",
    "scores_by_model(evals, height=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cruijff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
