# Workflow Integration Test Specification - Base Model Comparison
#
# This file defines a minimal, repeatable experiment for testing the complete
# workflow with base model comparison: design-experiment, scaffold-experiment,
# and run-experiment skills.
#
# Invocation: When user selects "Base vs Fine-tuned" test option

metadata:
  # Experiment naming pattern (actual name will include timestamp)
  name_pattern: "workflow_test_base_{date}"
  type: "sanity_check"  # Goes in ck-sanity-checks/
  scientific_question: >-
    Verify that the complete workflow (design, scaffold, run) executes correctly
    for comparing a base model against a fine-tuned model.
  purpose: >-
    Integration test to validate base model evaluation workflow works correctly
    alongside fine-tuning workflow.

tools:
  optimizer: "torchtune"
  evaluator: "inspect-ai"

models:
  - name: "Llama-3.2-1B-Instruct"
    purpose: "Fast training and evaluation for integration testing"

dataset:
  task: "capitalization"
  path: "data/green/capitalization/words_5L_80P_1000.json"
  format: "json"
  description: "5-letter words, 80% train split, 1000 samples"
  splits:
    train: 800
    validation: 100
    test: 100

training_runs:
  # Base model control (no training)
  - run_name: "Llama-3.2-1B-Instruct_base"
    model: "Llama-3.2-1B-Instruct"
    type: "control"

  # Single fine-tuned model
  - run_name: "Llama-3.2-1B-Instruct_rank4"
    model: "Llama-3.2-1B-Instruct"
    lora_rank: 4
    type: "fine-tuned"

evaluation:
  tasks:
    - name: "capitalization"
      script: "experiments/capitalization/inspect_task_capitalization.py"
      dataset: "same"  # Use same dataset as training
      description: "Tests word capitalization accuracy"

  epochs_to_evaluate: ["last"]  # Just final epoch for speed (epoch_0 after 1 epoch training)

  runs_to_evaluate: "all"  # All runs get evaluated on all tasks

common_config:
  # Training parameters
  epochs: 1
  batch_size: 4  # Small batch size works on 40GB GPUs (no constraint needed)
  lr: "1e-4"  # Standard default

  # System resources
  gpus: 1

  # Prompts
  system_prompt: "You are a helpful assistant."  # Simple, reliable prompt
  # prompt: Formats input for BOTH training AND evaluation (train/eval parity)
  # MUST include {input} placeholder. The inspect-ai eval reads this and applies same formatting.
  prompt: "Capitalize the given word: {input}\n"

  # Optional settings
  validation_during_training: true

expected_outputs:
  # What should exist after successful run
  directories:
    - "Llama-3.2-1B-Instruct_base/"
    - "Llama-3.2-1B-Instruct_rank4/"

  configs_per_run:
    - "setup_finetune.yaml"  # Even base model has this (though not used)
    - "eval/capitalization_epoch0.slurm"  # Base model gets evaluated

  configs_per_finetuned_run:  # Only fine-tuned runs have these
    - "finetune.yaml"
    - "finetune.slurm"

  logs_per_run:
    - "eval/logs/*.eval"  # Evaluation logs

  logs_per_finetuned_run:  # Only fine-tuned runs have these
    - "slurm-*.out"  # Training log

estimated_time:
  training: "~10 minutes"  # 1B model, 1 epoch, 1000 samples (only 1 run)
  evaluation: "~2 minutes"  # 2 runs × 1 eval × 1 epoch
  total: "~12 minutes"

validation_checks:
  # What to verify after execution
  - "All run directories created (base + fine-tuned)"
  - "Base model run has evaluation configs but no training configs"
  - "Fine-tuned run has both training and evaluation configs"
  - "setup_finetune.yaml contains correct lora_rank for fine-tuned run"
  - "Fine-tuning job completes successfully"
  - "Checkpoint directory exists for fine-tuned run (epoch_0/)"
  - "Evaluation jobs complete successfully for BOTH runs"
  - "Evaluation logs contain results for BOTH runs"
  - "Base model evaluation uses checkpoint_path pointing to HuggingFace model"

cleanup:
  # After successful test, these can be deleted to save space
  auto_delete: false  # Manual cleanup preferred
  keep_logs: true  # Always preserve logs for debugging
