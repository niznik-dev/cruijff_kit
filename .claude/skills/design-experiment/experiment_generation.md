# Experiment Generation

After user approves the plan, create the output files and suggest next steps.

## Files to Create

### 1. experiment_summary.yaml

**MANDATORY:** Before writing, READ `templates/experiment_summary.yaml` to understand the required structure. Do not freestyle â€” use the template schema exactly.

Create structured configuration in `{experiment_dir}/experiment_summary.yaml` using the template structure from `templates/experiment_summary.yaml`.

**Required sections:**
- `experiment`: name, question, date, hypothesis (optional), purpose (optional), directory
- `tools`: preparation, evaluation (for scaffold routing)
- `variables`: parameters that vary across runs (optional if only controls)
- `controls`: parameters held constant across all runs
- `models`: base model(s) with paths and sizes
- `data`: training dataset with splits
- `output`: checkpoint directories and naming patterns
- `runs`: complete list of fine-tuned + control runs
- `evaluation`: system prompt, temperature, scorer (structured list with optional params), tasks, matrix

**Important notes:**
- Use actual paths from `claude.local.md`, not placeholders
- **System prompt consistency is critical** - `evaluation.system_prompt` must match `controls.system_prompt` for inspect-ai
- **Prompt with {input} placeholder is critical**
- **Epochs are 0-indexed** - Use `[0, 1, 2]` in evaluation matrix for epochs 0, 1, 2
- Control runs: Set `epochs: null` in evaluation matrix (no epoch suffix)
- Fine-tuned runs: Specify epoch list `epochs: [0, 1]` for which epochs to evaluate

**YAML formatting:**
- Use standard YAML syntax (2-space indentation)
- Add helpful comments for clarity (e.g., `# Control run - no training`)
- Ensure proper list syntax: `[item1, item2]` or multi-line with `-`
- Quote strings with special characters or colons

For complete schema and examples, see `templates/experiment_summary.yaml`.

### 2. logs/design-experiment.log

Create human-readable audit trail in `{experiment_dir}/logs/design-experiment.log` that records all verification steps, calculations, and decisions.

For log format specification and examples, see `logging.md`.

**Key logging points:**
- `START_DESIGN`: Mark beginning of experiment design
- `VERIFY_MODEL`, `VERIFY_DATASET`, `VERIFY_EVAL_TASK`: Resource verification
- `SEARCH_PRIOR_RUNS`, `EXTRACT_TRAINING_SPEED`: Prior run analysis
- `CALCULATE_TRAINING_TIME`, `CALCULATE_DISK_USAGE`: Compute estimates
- `CHECK_DISK_SPACE`: Disk availability
- `GENERATE_RUNS`: Run generation from variables
- `GENERATE_EVAL_MATRIX`: Evaluation matrix creation
- `CREATE_YAML`: Write experiment_summary.yaml
- `COMPLETE_DESIGN`: Mark end of experiment design

**Log entry format:**
- Each entry starts with `[YYYY-MM-DD HH:MM:SS] ACTION_TYPE`
- Include `Details:`, action-specific key/value lines, and `Result:`
- Separate entries with blank lines
- Include `Error:` line for failures and warnings

### Files Generated by Downstream Skills

These files are NOT created by design-experiment, but are auto-generated by scaffold-experiment:

- `{run_dir}/setup_finetune.yaml` - Fine-tuning config (by scaffold-torchtune)
- `{run_dir}/eval/eval_config.yaml` - Eval config for base models (by scaffold-inspect)
- `{run_dir}/eval/{task}_epoch{N}.slurm` - Evaluation scripts (by scaffold-inspect)

---

## After Creation

### 1. Confirm Files Created

```
I've created the experiment plan at `{experiment_dir}/experiment_summary.yaml`.

All verification steps and calculations have been logged in `{experiment_dir}/logs/design-experiment.log`.
```

### 2. Ask About Next Steps

```
Would you like me to proceed with scaffolding? I can run `scaffold-experiment` to generate all configs.
```

### 3. Explain Workflow Options

**Automated workflow (recommended):**
- Run `scaffold-experiment` skill to generate:
  - Fine-tuning configs via `scaffold-torchtune` (finetune.yaml, finetune.slurm)
  - Evaluation configs via `scaffold-inspect` (inspect.slurm, task scripts)
- Run `run-experiment` skill to execute:
  - Fine-tuning via `run-torchtune` (submit jobs, monitor progress)
  - Evaluation via `run-inspect` (submit jobs after training completes, monitor progress)
- Run `analyze-experiment` skill to interpret results (planned)

**Manual workflow (if needed):**
- User can manually create directories and configs
- Follow the experiment plan as documented in experiment_summary.yaml

---

## Conversation Pattern

```
Perfect! I've created:
- experiment_summary.yaml with the complete plan
- design-experiment.log with all verification steps and calculations

Would you like me to proceed with scaffolding? I can run `scaffold-experiment` to generate all the configs and SLURM scripts for you.
```

---

## Prerequisites Handling

If some resources were missing during verification (e.g., evaluation task scripts don't exist yet):

```markdown
## Prerequisites

Before running `scaffold-experiment`, you must:

1. **Create evaluation task:** Run `create-inspect-task` to create the capitalization task script
   - Task name: capitalization
   - Expected location: `{repo_dir}/experiments/capitalization/cap_task.py`

Once prerequisites are complete, you can proceed with scaffolding.
```

Don't block the plan - document prerequisites clearly so user knows what to do next.
