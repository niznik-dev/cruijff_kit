# setup_finetune.yaml template
# Populate this structure with run-specific values

# Run identification
my_wandb_project: {from claude.local.md or experiment-level project name}
my_wandb_run_name: {directory_name, e.g., "r8_lr1e-5"}

# Directory Configuration (for dataset path construction)
input_dir_base: {parent directory of dataset from experiment_summary}
input_formatting: ''  # Usually empty string

# Dataset Configuration
dataset_label: {extracted from dataset filename, e.g., "words_4L_80P_300"}
dataset_ext: {extracted from dataset filename, e.g., ".json"}

# Model Configuration
torchtune_model_name: {from experiment_summary.md Resources â†’ Models, e.g., "Llama-3.2-1B-Instruct"}
# model_checkpoint: {optional - only if directory name differs from torchtune_model_name}

# Prompt Template - CRITICAL for train/eval parity
# This field formats the input for BOTH training AND evaluation.
# The inspect-ai eval task reads this field and applies the same formatting.
#
# Requirements:
#   - MUST include {input} placeholder (will be replaced with the input field value)
#   - SHOULD end with \n to separate prompt from expected output
#
# Examples:
#   - "Capitalize: {input}\n"           # Instruction + input + newline
#   - "{input}\n"                       # Raw input + newline (for tasks without instruction)
#   - "Question: {input}\nAnswer: "    # Multi-line prompt
#
# WARNING: An empty prompt ('') means the input field is NEVER shown to the model!
prompt: {from experiment_summary.md Configuration, e.g., "Capitalize the given word: {input}\n"}

# Dataset Type (optional - defaults to chat_completion)
# chat_completion: Uses HuggingFace apply_chat_template() for train/eval parity (RECOMMENDED)
# conditional_completion: DEPRECATED - raw string concatenation, may cause eval mismatches
# dataset_type: chat_completion  # Uncomment only if you need to override

# Hyperparameters (run-specific)
lora_rank: {from run table}
lr: {from run table, format as 1e-5 or 5e-5}  # NOTE: parameter is 'lr' not 'learning_rate'
batch_size: {from run table or common config}

# Training configuration (common across runs)
epochs: {from experiment_summary.md Configuration}
log_every_n_steps: {use template default, typically 1}
run_val_every_n_steps: {use template default, typically 0}

# Checkpoint Options
stash_adapter_weights: 'true'  # From template default

# Output configuration
output_dir_base: {from claude.local.md}
conda_env: {from claude.local.md}

# SLURM configuration (optional - only if specified in claude.local.md)
account: {from claude.local.md SLURM Defaults, if present}

# Custom Recipe
custom_recipe: {from template, e.g., cruijff_kit.tools.torchtune.custom_recipes.lora_finetune_single_device_stable}
