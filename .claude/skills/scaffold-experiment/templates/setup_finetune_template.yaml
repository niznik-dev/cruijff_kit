# setup_finetune.yaml template
# Populate this structure with run-specific values

# Run identification
my_wandb_project: {from claude.local.md or experiment-level project name}
my_wandb_run_name: {directory_name, e.g., "r8_lr1e-5"}

# Directory Configuration (for dataset path construction)
input_dir_base: {parent directory of dataset from experiment_summary}
input_formatting: ''  # Usually empty string

# Dataset Configuration
dataset_label: {extracted from dataset filename, e.g., "words_4L_80P_300"}
dataset_ext: {extracted from dataset filename, e.g., ".json"}

# Model Configuration
torchtune_model_name: {from experiment_summary.md Resources â†’ Models, e.g., "Llama-3.2-1B-Instruct"}
# model_checkpoint: {optional - only if directory name differs from torchtune_model_name}

# Prompt Template (use {input} placeholder for the input field)
# Format: "instruction: {input}\n" - the colon separator works better than newlines
prompt: {from experiment_summary.md Configuration, e.g., "Capitalize the given word: {input}\n"}

# Hyperparameters (run-specific)
lora_rank: {from run table}
lr: {from run table, format as 1e-5 or 5e-5}  # NOTE: parameter is 'lr' not 'learning_rate'
batch_size: {from run table or common config}

# Training configuration (common across runs)
epochs: {from experiment_summary.md Configuration}
log_every_n_steps: {use template default, typically 1}
run_val_every_n_steps: {use template default, typically 0}

# Checkpoint Options
stash_adapter_weights: 'true'  # From template default

# Output configuration
output_dir_base: {from claude.local.md}
conda_env: {from claude.local.md}

# SLURM configuration (optional - only if specified in claude.local.md)
account: {from claude.local.md SLURM Defaults, if present}

# Custom Recipe
custom_recipe: {from template, e.g., cruijff_kit.tools.torchtune.custom_recipes.lora_finetune_single_device_stable}
