# Model Evaluators - Meta-Organization Pattern

This directory contains tool-specific modules for model evaluation (e.g., inspect-ai, custom evaluation frameworks).

## Standard Workflow Stages

Each evaluator tool should follow this four-stage pattern:

### Stage 1: INPUT PROCESSING
**File:** `parsing.md`
**Purpose:** Extract structured data from input files (experiment_summary.md, claude.local.md)
**Outputs:** Parsed evaluation tasks, datasets, and configuration

### Stage 2: PLANNING / SELECTION
**Files:** `*_selection.md`
**Purpose:** Make decisions and apply algorithms without creating artifacts
**Examples:**
- `scenario_selection.md` - Choose evaluation configuration approach (base model vs fine-tuned vs custom dataset)
- `metric_selection.md` - Select which metrics to compute
- `dataset_selection.md` - Choose which datasets to evaluate on

### Stage 3: ARTIFACT CREATION
Two distinct approaches for creating artifacts:

#### Manual Generation
**Files:** `*_generation.md`
**Purpose:** Create files directly (Claude writes the content)
**Examples:**
- `directory_generation.md` - Create eval/ subdirectories using mkdir
- `slurm_generation.md` - Write evaluation SLURM scripts directly
- `config_generation.md` - Write evaluation config files directly

#### Script Execution
**File:** `script_execution.md`
**Purpose:** Run external repeatable scripts that generate artifacts
**Examples:**
- Running evaluation setup scripts
- Executing tool-specific CLIs for config generation
- Invoking preprocessing pipelines

**Key Distinction:** Generation = Claude creates content; Execution = Claude runs external tools

### Stage 4: VERIFICATION
**File:** `validation.md`
**Purpose:** Verify evaluation setup is correct and complete
**Checks:** Task scripts exist, paths are valid, expected functions are present

## Tool Structure Template

When adding a new evaluator tool, use this structure:

```
evaluators/
└── {tool-name}/
    ├── main.md                      # Tool overview and entry point
    ├── parsing.md                   # Stage 1: Input processing
    ├── {decision}_selection.md      # Stage 2: Planning/decision logic
    ├── {artifact}_generation.md     # Stage 3a: Create files directly
    ├── script_execution.md          # Stage 3b: Run external scripts (if needed)
    └── validation.md                # Stage 4: Verification
```

## Current Tools

### inspect
Evaluating models using inspect-ai framework.

**Workflow:**
1. `parsing.md` - Extract evaluation tasks and configuration from experiment_summary.md
2. `scenario_selection.md` - Choose evaluation approach (config_path vs explicit params)
3. `slurm_generation.md` - Generate evaluation SLURM scripts (manual)
4. `validation.md` - Verify task scripts exist and contain expected functions

## Design Principles

**Separation of Concerns:** Selection logic (planning) is separate from creation logic (generation/execution)

**Manual vs External:** Distinguish between files Claude creates directly vs files generated by external scripts

**Reusability:** Generic stage names can be reused across different tools

**Progressive Disclosure:** SKILL.md links to main.md, which links to specific stage files

**Consistency:** All tools follow the same 4-stage pattern for predictable organization

## Future Tools

When adding new evaluators (custom frameworks, benchmark suites, etc.), follow this same pattern:
- Keep the 4-stage structure
- Use the `*_selection.md` naming for decision logic
- Use the `*_generation.md` naming for manual file creation
- Use `script_execution.md` for running external tools
- Document tool-specific details in `main.md`
