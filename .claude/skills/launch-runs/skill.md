# Launch Runs

You are helping the user submit fine-tuning jobs for a planned experiment.

**CURRENT SCOPE**: This skill focuses on **fine-tuning job submission only**.

**AUTOMATION AVAILABLE**: See [Automated Submission Tools](#automated-submission-tools) section for utilities that simplify batch submission and status updates.

**TODO - Future Enhancement**: Improve evaluation job handling with:
- Automated eval script mapping (task name → script path)
- Cross-evaluation support (models evaluated on multiple datasets)
- Checkpoint discovery and validation
- Base model evaluation handling
- See improvement notes at end of file

## Your Task

Submit fine-tuning SLURM jobs and track their status in `runs_status.yaml`.

## Prerequisites

Before using this skill, verify the following exist:
- `runs_plan.md`: Complete experiment plan
- `runs_status.yaml`: Status tracking file
- Individual run directories with generated configs
- Each run directory must contain:
  - `finetune.yaml` ✓ (generated by setup_finetune.py)
  - `finetune.slurm` ✓ (generated by setup_finetune.py)

## Workflow

### 1. Verify Prerequisites

**Check directory structure:**
```bash
# From experiment directory (e.g., /scratch/.../cap_cross_eval_5_9_13L_2025-10-19/)
ls -la  # Should show run directories and runs_status.yaml
```

**Verify run directories have required files:**
```bash
# For each pending run, check:
ls {run_name}/finetune.yaml
ls {run_name}/finetune.slurm
```

If any files are missing:
- Report which runs are incomplete
- Suggest: "Run setup-experiment-dirs skill first"
- Do NOT attempt to submit incomplete runs

### 2. Read Current Status

**Load runs_status.yaml:**
- Parse the YAML to identify runs where `finetune.status == 'pending'`
- Count total pending runs
- Group by model size if helpful (e.g., "6 × 1B models, 6 × 3B models")

**Check current cluster queue:**
```bash
squeue -u $USER --format="%.10i %.30j %.8T %.10M %.10l %.8N"
```
- Report number of jobs already running/pending
- This helps user decide batch size

### 3. Show Dry-Run Preview

**ALWAYS show what will be submitted before doing it:**

```
=== DRY RUN: Fine-tuning Jobs ===
Will submit 12 jobs to SLURM:

1B Models (~3 min each):
  1. Llama-3.2-1B-Instruct_5L_rank4
  2. Llama-3.2-1B-Instruct_5L_rank64
  3. Llama-3.2-1B-Instruct_9L_rank4
  4. Llama-3.2-1B-Instruct_9L_rank64
  5. Llama-3.2-1B-Instruct_13L_rank4
  6. Llama-3.2-1B-Instruct_13L_rank64

3B Models (~14 min each):
  7. Llama-3.2-3B-Instruct_5L_rank4
  8. Llama-3.2-3B-Instruct_5L_rank64
  9. Llama-3.2-3B-Instruct_9L_rank4
  10. Llama-3.2-3B-Instruct_9L_rank64
  11. Llama-3.2-3B-Instruct_13L_rank4
  12. Llama-3.2-3B-Instruct_13L_rank64

Estimated wall-clock time:
- Parallel (12 GPUs): ~14 min
- Sequential (1 GPU): ~102 min

Proceed with submission? (yes/no)
```

### 4. Submit Jobs

**For each pending run:**

```bash
# Navigate to run directory
cd /scratch/gpfs/MSALGANIK/mjs3/{run_group}/{run_name}

# Submit job and capture output
output=$(sbatch finetune.slurm 2>&1)

# Check if submission succeeded
if [[ $output =~ Submitted\ batch\ job\ ([0-9]+) ]]; then
  job_id="${BASH_REMATCH[1]}"
  echo "✓ {run_name} → Job $job_id"
  # Update YAML (see section 5)
else
  echo "✗ {run_name} → FAILED: $output"
  # Do NOT update YAML for failed submissions
fi
```

**Error handling:**
- If `sbatch` fails, capture the error message
- Do NOT update status to "submitted" for failed jobs
- Common errors:
  - "Invalid account": Check SLURM account in finetune.slurm
  - "Invalid partition": Run `sinfo` to check available partitions
  - "Permission denied": Check file permissions
- Continue with remaining jobs, report failures at end

**Progress reporting:**
Show progress during submission:
```
Submitting fine-tuning jobs...
[1/12] ✓ Llama-3.2-1B-Instruct_5L_rank4 → Job 1234567
[2/12] ✓ Llama-3.2-1B-Instruct_5L_rank64 → Job 1234568
[3/12] ✓ Llama-3.2-1B-Instruct_9L_rank4 → Job 1234569
...
```

### 5. Update Status File

**After each successful submission:**

Update `runs_status.yaml` with:
- Status: `submitted`
- Job ID from sbatch output
- Output file path: `{run_dir}/slurm-{job_id}.out`
- Timestamp in ISO 8601 format: `YYYY-MM-DD HH:MM:SS`

**Timestamp format:**
```python
from datetime import datetime
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

**Update format:**
```yaml
runs:
  Llama-3.2-1B-Instruct_5L_rank4:
    finetune:
      status: submitted
      job_id: 1234567
      output: /scratch/gpfs/MSALGANIK/mjs3/cap_cross_eval_5_9_13L_2025-10-19/Llama-3.2-1B-Instruct_5L_rank4/slurm-1234567.out
      last_updated: "2025-10-19 14:30:00"
```

**IMPORTANT:**
- Read entire YAML → modify → write (atomic update)
- Use proper YAML formatting (preserve structure)
- Only update runs that were successfully submitted
- Keep existing data for other runs unchanged

### 6. Report Summary

**After all submissions complete, show summary:**

```
=== Submission Summary ===
✓ Successfully submitted: 12/12 jobs
✗ Failed: 0

Job IDs: 1234567-1234578

All job IDs saved to: runs_status.yaml

Quick check:
  squeue -u mjs3

View all job IDs from status file:
  grep "job_id:" /scratch/gpfs/MSALGANIK/mjs3/cap_cross_eval_5_9_13L_2025-10-19/runs_status.yaml | grep -v "null"

Next steps:
1. Verify jobs are queued: squeue -u mjs3
2. Safe to disconnect - jobs will continue running
3. Reconnect later and use /monitor-jobs to check status
4. Check logs: tail -f /scratch/.../Llama-3.2-1B-Instruct_5L_rank4/slurm-1234567.out

Updated: runs_status.yaml
```

**If there were failures:**
```
=== Submission Summary ===
✓ Successfully submitted: 10/12 jobs
✗ Failed: 2

Failed runs:
  - Llama-3.2-3B-Instruct_13L_rank4: Invalid partition
  - Llama-3.2-3B-Instruct_13L_rank64: Invalid partition

Suggest checking finetune.slurm files for failed runs.
```

### 7. Check Job Status (Optional)

**User can request status update after submission:**

```bash
squeue -u $USER --format="%.10i %.30j %.8T %.10M %.10l %.8N"
```

**Parse output and report:**
```
=== Current Job Status ===
PENDING: 8 jobs (waiting for resources)
RUNNING: 4 jobs
COMPLETED: 0 jobs
FAILED: 0 jobs

Longest running job: 5 minutes (Llama-3.2-3B-Instruct_5L_rank4)
```

**Optionally update runs_status.yaml:**
- Jobs in RUNNING state → status: `running`
- Jobs completed successfully → status: `completed`
- Jobs failed → status: `failed`

## Status Values

**Valid status values for `finetune`:**
- `pending`: Not yet submitted
- `submitted`: Job submitted to SLURM (queued or running)
- `running`: Job is currently executing (optional, use if actively monitoring)
- `completed`: Job finished successfully
- `failed`: Job failed with errors
- `skipped`: Intentionally not running (e.g., base model - no fine-tuning needed)

## Interactive Options

**When user invokes this skill, ask:**

1. **Launch all pending fine-tuning jobs?**
   - Submits all runs where `finetune.status == 'pending'`
   - Shows dry-run preview first
   - Updates status file with all job IDs

2. **Launch specific runs only?**
   - User provides list of run names (comma-separated or space-separated)
   - Only submit those specific jobs
   - Example: "Llama-3.2-1B-Instruct_5L_rank4, Llama-3.2-1B-Instruct_9L_rank4"

3. **Check job status and update tracking file?**
   - Query SLURM queue with `squeue`
   - Update `runs_status.yaml` with current states
   - Report summary to user

4. **Resubmit failed jobs?**
   - Identify jobs with `status == 'failed'`
   - Ask user if they want to resubmit
   - Reset status to pending and resubmit

## Best Practices

**Job submission:**
- ALWAYS show dry-run preview before submitting
- Capture job ID from sbatch output using regex: `Submitted batch job (\d+)`
- Verify submission succeeded before updating status
- Continue with remaining jobs if one fails
- **For cache collision prevention**: Use sequential submission with delays for jobs sharing datasets

**Status tracking:**
- **RECOMMENDED**: Use `tools/run_management/update_run_status.py` for status updates (single atomic operation)
- **Alternative**: Multiple Edit tool calls (works but more verbose)
- Use ISO 8601 timestamp format: `YYYY-MM-DD HH:MM:SS`
- Read → modify → write entire YAML atomically
- Preserve existing data for other runs

**User communication:**
- Show clear summary of pending runs before acting
- Report job IDs immediately after submission
- Provide monitoring commands (squeue, tail, etc.)
- Report any failures clearly with error messages

## Automated Submission Tools

**Location**: `tools/run_management/`

These utilities simplify batch submission and status updates:

### submit_pending_runs.sh

Automates the entire submission workflow - finds pending runs, verifies configs, submits jobs, and updates status file.

**Usage:**
```bash
# Parallel submission (fast but risks cache collisions)
./tools/run_management/submit_pending_runs.sh /path/to/experiment

# Sequential submission (safe for shared datasets)
./tools/run_management/submit_pending_runs.sh --sequential /path/to/experiment

# Dry run (preview without submitting)
./tools/run_management/submit_pending_runs.sh --dry-run /path/to/experiment

# Submit specific runs only
./tools/run_management/submit_pending_runs.sh \
    --only Llama-3.2-1B-Instruct_5L_rank4 \
    --only Llama-3.2-1B-Instruct_5L_rank64 \
    /path/to/experiment
```

**Benefits:**
- Single command replaces entire workflow
- Automatic status file updates
- Built-in pre-flight verification
- Progress indicators
- Error handling with clear reports
- Sequential mode prevents cache collisions

### update_run_status.py

Atomically update runs_status.yaml with job IDs and timestamps.

**Usage:**
```bash
# Single update
python tools/run_management/update_run_status.py \
    --status-file /path/to/runs_status.yaml \
    --run-name Llama-3.2-1B-Instruct_5L_rank4 \
    --job-id 1234567 \
    --status submitted

# Batch update
python tools/run_management/update_run_status.py \
    --status-file /path/to/runs_status.yaml \
    --batch job_updates.json
```

**Benefits:**
- Replaces multiple Edit tool calls with single command
- Atomic YAML updates
- Automatic timestamp generation
- Auto-generates output paths
- Validates status values

**When to use:**
- **Automated script**: Use when available - faster and more reliable
- **Manual Edit calls**: Fallback when script unavailable or for single updates

See: [tools/run_management/README.md](/home/mjs3/cruijff_kit/tools/run_management/README.md) for complete documentation.

## Cache Collision Prevention

**Problem**: Multiple jobs loading the same HuggingFace dataset simultaneously can corrupt the cache, causing `FileNotFoundError`.

**Solution**: Use sequential submission with delays:

```bash
./tools/run_management/submit_pending_runs.sh --sequential --delay 10 /path/to/experiment
```

**Why this works:**
- First job populates the cache
- Subsequent jobs (after delay) reuse the cached data safely
- 10-second delay is usually sufficient

**When to use:**
- Jobs trained on the same dataset (e.g., all `words_5L` runs)
- Multiple models of different sizes using same data
- Resubmitting failed jobs

**When NOT needed:**
- Jobs using completely different datasets
- Datasets already cached from previous runs
- Single job submission

## Example Session

```
User: Launch fine-tuning jobs
Assistant: I'll check which fine-tuning jobs are pending...

Found 12 pending fine-tuning jobs in runs_status.yaml.

=== DRY RUN: Fine-tuning Jobs ===
Will submit 12 jobs to SLURM:

1B Models (~3 min each):
  1. Llama-3.2-1B-Instruct_5L_rank4
  2. Llama-3.2-1B-Instruct_5L_rank64
  3. Llama-3.2-1B-Instruct_9L_rank4
  4. Llama-3.2-1B-Instruct_9L_rank64
  5. Llama-3.2-1B-Instruct_13L_rank4
  6. Llama-3.2-1B-Instruct_13L_rank64

3B Models (~14 min each):
  7. Llama-3.2-3B-Instruct_5L_rank4
  8. Llama-3.2-3B-Instruct_5L_rank64
  9. Llama-3.2-3B-Instruct_9L_rank4
  10. Llama-3.2-3B-Instruct_9L_rank64
  11. Llama-3.2-3B-Instruct_13L_rank4
  12. Llama-3.2-3B-Instruct_13L_rank64

Estimated wall-clock time:
- Parallel (12 GPUs): ~14 min
- Sequential (1 GPU): ~102 min

Proceed with submission? (yes/no)

User: yes
Assistant: Submitting jobs...
[1/12] ✓ Llama-3.2-1B-Instruct_5L_rank4 → Job 1234567
[2/12] ✓ Llama-3.2-1B-Instruct_5L_rank64 → Job 1234568
[3/12] ✓ Llama-3.2-1B-Instruct_9L_rank4 → Job 1234569
[4/12] ✓ Llama-3.2-1B-Instruct_9L_rank64 → Job 1234570
[5/12] ✓ Llama-3.2-1B-Instruct_13L_rank4 → Job 1234571
[6/12] ✓ Llama-3.2-1B-Instruct_13L_rank64 → Job 1234572
[7/12] ✓ Llama-3.2-3B-Instruct_5L_rank4 → Job 1234573
[8/12] ✓ Llama-3.2-3B-Instruct_5L_rank64 → Job 1234574
[9/12] ✓ Llama-3.2-3B-Instruct_9L_rank4 → Job 1234575
[10/12] ✓ Llama-3.2-3B-Instruct_9L_rank64 → Job 1234576
[11/12] ✓ Llama-3.2-3B-Instruct_13L_rank4 → Job 1234577
[12/12] ✓ Llama-3.2-3B-Instruct_13L_rank64 → Job 1234578

=== Submission Summary ===
✓ Successfully submitted: 12/12 jobs
✗ Failed: 0

Job IDs: 1234567-1234578

Next steps:
1. Monitor jobs: squeue -u mjs3
2. Check specific log: tail -f /scratch/gpfs/MSALGANIK/mjs3/cap_cross_eval_5_9_13L_2025-10-19/Llama-3.2-1B-Instruct_5L_rank4/slurm-1234567.out
3. Wait for completion, then rerun /launch-runs for evaluations

Updated: runs_status.yaml
```

## Notes

- This skill works with the status file created by `plan-runs` and `setup-experiment-dirs`
- Can be run multiple times safely (idempotent - won't resubmit jobs that aren't pending)
- Integrates with `monitor-jobs` skill for ongoing tracking
- Integrates with `update-run-status` skill for status synchronization

## Disconnecting and Long-Running Jobs

**IMPORTANT**: Once jobs are submitted to SLURM, they run independently of your session.

### You Can Safely Disconnect

After submitting jobs with this skill, you can:
- Close Claude Code
- Disconnect from SSH/HPC
- Shut down your local computer
- Sleep, travel, etc.

**Jobs will continue running on the HPC cluster.**

SLURM jobs are:
- Managed by the cluster's scheduler (independent of login sessions)
- Executed on compute nodes (not your login node)
- Persistent until completion or cancellation

### Workflow for Overnight Runs

**Evening (before disconnect):**
1. Submit all fine-tuning jobs with this skill
2. Note the job ID range (e.g., 1234567-1234578)
3. Verify jobs appear in queue: `squeue -u mjs3`
4. Disconnect - jobs will continue

**Morning (after reconnect):**
1. Retrieve job IDs from status file (no need to remember them):
   ```bash
   cd /scratch/gpfs/MSALGANIK/mjs3/cap_cross_eval_5_9_13L_2025-10-19
   grep "job_id:" runs_status.yaml | grep -v "null"
   ```

2. Check which jobs completed:
   ```bash
   sacct -u mjs3 -S today --format=JobID,JobName,State,ExitCode,Elapsed,End -X
   ```

3. Check job queue for any still running:
   ```bash
   squeue -u mjs3
   ```

4. Update runs_status.yaml to sync with actual SLURM state:
   - Use `/monitor-jobs` or `/update-run-status` skills
   - These will update status from pending/submitted → running/completed/failed

5. Check output logs for any issues:
   ```bash
   # Check specific log
   tail -50 /scratch/.../Llama-3.2-1B-Instruct_5L_rank4/slurm-1234567.out

   # Check for errors across all logs
   grep -i "error\|failed\|exception" /scratch/.../*/slurm-*.out
   ```

6. Launch evaluation jobs for completed runs (when evaluation support is added)

### Key Files

**SLURM output logs** (created automatically):
```
/scratch/gpfs/MSALGANIK/mjs3/cap_cross_eval_5_9_13L_2025-10-19/
  Llama-3.2-1B-Instruct_5L_rank4/slurm-1234567.out
  Llama-3.2-1B-Instruct_5L_rank64/slurm-1234568.out
  ...
```

**Status tracking** (updated by skills):
```
/scratch/gpfs/MSALGANIK/mjs3/cap_cross_eval_5_9_13L_2025-10-19/runs_status.yaml
```

**Note**: `runs_status.yaml` may become stale if you disconnect. Use `monitor-jobs` or `update-run-status` skills to re-sync with actual SLURM state when you reconnect.

### Common Commands for Checking Status

```bash
# Currently running/pending jobs
squeue -u mjs3 --format="%.10i %.30j %.8T %.10M %.10l %.8N"

# Jobs from last 24 hours (including completed)
sacct -u mjs3 -S $(date -d '1 day ago' +%Y-%m-%d) \
  --format=JobID,JobName,State,ExitCode,Elapsed,End -X

# Failed jobs only
sacct -u mjs3 -S today --format=JobID,JobName,State,ExitCode -X | grep FAILED

# Check resource usage for completed job
seff 1234567  # Replace with actual job ID
```

---

## FUTURE ENHANCEMENTS NEEDED

**The following improvements are needed for evaluation job submission:**

### 1. Evaluation Script Mapping
**Problem**: The status file has task names like `cap_5L_epoch_0`, but we need to map these to actual eval script paths.

**Solution**:
```python
import re

def map_task_to_eval_script(task_name, task_base_dir):
    """Map task name from runs_status.yaml to eval script path.

    Examples:
      cap_5L_epoch_0 → /home/mjs3/cruijff_kit/tasks/capitalization/eval_5L.py
      cap_9L → /home/mjs3/cruijff_kit/tasks/capitalization/eval_9L.py
    """
    match = re.match(r"([^_]+)_(\w+)(?:_epoch_(\d+))?", task_name)
    if not match:
        raise ValueError(f"Cannot parse task name: {task_name}")

    task_prefix = match.group(1)  # "cap"
    word_length = match.group(2)  # "5L"
    epoch_num = match.group(3)    # "0" or None (for base models)

    eval_script = f"{task_base_dir}/eval_{word_length}.py"
    return eval_script, epoch_num
```

### 2. Cross-Evaluation Support
**Problem**: Models trained on one dataset need to be evaluated on multiple datasets (cross-evaluation).

**Example**:
- Model `Llama-3.2-1B-Instruct_5L_rank4` trained on 5-letter words
- Must evaluate on: `eval_5L.py`, `eval_9L.py`, `eval_13L.py`
- Each evaluation for 2 epochs: epoch_0, epoch_1

**Solution**: For each (run, task, epoch) combination in runs_status.yaml:
1. Extract task name (e.g., `cap_9L_epoch_0`)
2. Map to eval script (→ `eval_9L.py`)
3. Find checkpoint dir (→ `{run_dir}/epoch_0/`)
4. Generate inspect.slurm with correct paths
5. Submit and track

### 3. Checkpoint Discovery
**Problem**: Need to verify checkpoints exist before launching evals.

**Solution**:
```bash
# For a given run directory
run_dir="/scratch/.../Llama-3.2-1B-Instruct_5L_rank4"
epoch_dir="$run_dir/epoch_0"

# Verify checkpoint files exist
if [[ -f "$epoch_dir/adapter_model.safetensors" ]] && \
   [[ -f "$epoch_dir/adapter_config.json" ]]; then
    echo "✓ Checkpoint found for epoch_0"
else
    echo "✗ Checkpoint missing for epoch_0"
    # Mark evaluation as "blocked" in runs_status.yaml
fi
```

### 4. Base Model Evaluation
**Problem**: Base models use different flags and don't have epochs.

**Differences**:
- Fine-tuned: `--finetune_epoch_dir {run_dir}/epoch_0`
- Base model: `--base_model_dir /scratch/gpfs/MSALGANIK/pretrained-llms/Llama-3.2-1B-Instruct`
- Base models evaluated once per task (no epoch suffix in status YAML)

**Solution**: Detect base model runs (status: `skipped` for finetune) and use appropriate flags.

### 5. Batch Evaluation Submission
**Problem**: 78 evaluation jobs need structured batching.

**Solution**: Similar to fine-tuning submission, but:
1. Group by epoch (submit all epoch_0 evals first, then epoch_1)
2. Or group by model (submit all evals for one model, then next)
3. Show dry-run with count per category

### 6. Blocked Status
**Problem**: If checkpoint doesn't exist yet (training still running), evaluation can't start.

**Solution**: Add new status value:
```yaml
evaluations:
  cap_5L_epoch_0:
    status: blocked
    reason: "Waiting for checkpoint at epoch_0/"
    last_updated: "2025-10-19 15:00:00"
```

Once checkpoint appears, user reruns skill to submit.

### 7. Output Directory Calculation
**Problem**: Need to generate correct `--output_subdir` path for each evaluation.

**Pattern**:
- Fine-tuned: `evaluations/cap_5L_epoch_0`
- Base model: `evaluations/cap_5L`

**Solution**: Extract from runs_status.yaml task name directly.

---

**When evaluation support is added, update the skill header to remove the "TODO" note and update the scope description.**
