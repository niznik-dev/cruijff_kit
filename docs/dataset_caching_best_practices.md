# Dataset Caching Best Practices

**Last Updated**: 2025-10-20

This document provides guidance on HuggingFace dataset caching strategies for HPC environments to avoid cache collisions and ensure reliable job execution.

---

## TL;DR

**Use per-job cache directories for reliability:**
```bash
export HF_DATASETS_CACHE="/scratch/gpfs/$USER/.cache/hf-datasets-${SLURM_JOB_ID}"
```

The SLURM scripts generated by `tools/slurm/generate_slurm_scripts.py` include this by default.

---

## The Problem: Cache Collisions

### What Happens
When multiple jobs try to load the same dataset simultaneously, they may attempt to create the same cache files concurrently, leading to:
- **Corrupted cache files**: Partial writes, race conditions
- **File not found errors**: Cache directory created but files missing
- **Permission errors**: File locks or permission conflicts

### Why It Happens
The HuggingFace `datasets` library:
1. Computes a hash of the dataset configuration
2. Creates a cache directory based on this hash
3. Converts source data (JSON, CSV, etc.) to Arrow format
4. Stores in `~/.cache/huggingface/datasets/` by default

When multiple processes do this simultaneously, **race conditions** can occur.

### Real Example (2025-10-20)
In experiment `cap_cross_eval_5_9_13L_2025-10-20`:
- 4 jobs using `words_9L_80P_10000.json` all failed with "Can't read json-train.arrow"
- Jobs using other datasets (5L, 13L) succeeded with shared cache
- Pre-caching attempts failed mysteriously
- **Solution**: Per-job cache directories eliminated all failures

---

## Solution 1: Per-Job Cache (Recommended for Most Cases)

### How It Works
Each SLURM job gets its own isolated cache directory using the job ID:
```bash
export HF_DATASETS_CACHE="/scratch/gpfs/$USER/.cache/hf-datasets-${SLURM_JOB_ID}"
mkdir -p "${HF_DATASETS_CACHE}"
```

After training completes, clean up:
```bash
rm -rf "${HF_DATASETS_CACHE}"
```

### Pros & Cons
✅ **Pros**:
- Guaranteed isolation (no collisions possible)
- Simple to implement (one environment variable)
- Easy to debug (each job independent)
- Automatic cleanup saves disk space

⚠️ **Cons**:
- Each job rebuilds cache (~30-60 sec overhead for small datasets)
- Duplicate storage during job execution
- Not ideal for very large datasets (multi-GB)

### When to Use
- **Small to medium datasets** (< 1GB): Overhead is negligible
- **Unpredictable job scheduling**: Can't control when jobs start
- **Reliability is critical**: Can't afford cache-related failures
- **Quick experiments**: Don't want to debug cache issues

### Cost Analysis
For typical datasets in cruijff_kit:
- **Storage**: ~10-100MB per job (deleted after completion)
- **Time**: ~30-60 seconds per job
- **Trade-off**: 5-8% overhead for 100% reliability

**This is the default in cruijff_kit SLURM scripts.**

---

## Solution 2: Parquet Format (Best Long-Term)

### Why Parquet?
- More stable caching behavior
- Faster loading times
- Better compression
- Industry standard for ML datasets
- Native support in HuggingFace datasets

### Conversion Process
```python
from datasets import load_dataset

# Load from JSON
ds = load_dataset("json", data_files={
    "train": "input/dataset.json",
    "validation": "input/dataset.json",
    "test": "input/dataset.json"
}, field="train")  # or appropriate field

# Save as Parquet
ds.save_to_disk("input/dataset_parquet/")
```

### Update Config
```yaml
# Before (JSON)
dataset:
  _component_: torchtune.datasets.instruct_dataset
  source: json
  data_files:
    train: ${input_dir}/${dataset_label}.json
  field: train  # or split: train

# After (Parquet)
dataset:
  _component_: torchtune.datasets.instruct_dataset
  source: datasets.load_from_disk
  path: ${input_dir}/${dataset_label}_parquet/
  split: train
```

### When to Use
- **Production pipelines**: Datasets used repeatedly
- **Large datasets**: Multi-GB datasets where caching overhead matters
- **Long-term projects**: Worth the one-time conversion effort
- **Shared datasets**: Multiple users/experiments using same data

---

## Solution 3: Centralized Pre-Built Cache (Advanced)

### Approach
1. Build cache once in shared location
2. Make it read-only
3. All jobs reference the shared cache

### Implementation
```bash
# Step 1: Build cache (one-time)
export HF_DATASETS_CACHE=/scratch/shared/hf-cache
python scripts/build_all_caches.py

# Step 2: Make read-only
chmod -R a-w /scratch/shared/hf-cache

# Step 3: Jobs use shared cache
export HF_DATASETS_CACHE=/scratch/shared/hf-cache
```

### Challenges
- **Verification**: Must ensure cache is complete before jobs start
- **Read-only issues**: Some versions of datasets library may expect write access
- **Mysterious failures**: In testing, pre-caching didn't prevent failures (investigate further)
- **Maintenance**: Need to rebuild cache when datasets change

### When to Use
- **Very large datasets**: Where per-job caching is too expensive
- **Many jobs on same dataset**: 100+ jobs using identical data
- **Controlled environment**: Can guarantee cache is built before job submission

---

## Solution 4: Staggered Submission (Not Recommended)

### Approach
Submit jobs sequentially or with SLURM dependencies to avoid simultaneous starts.

### Why Not Recommended
- **Still relies on shared cache** (which may have underlying issues)
- **Slower overall**: Serializes job starts
- **Doesn't address root cause**: Mysterious pre-caching failures
- **Per-job cache is simpler and more reliable**

---

## Decision Matrix

| Use Case | Dataset Size | Jobs | Recommendation |
|----------|-------------|------|----------------|
| Quick experiment | < 1GB | < 20 | **Per-job cache** |
| Regular workflow | < 1GB | Any | **Parquet format** |
| Large dataset | > 1GB | < 10 | **Per-job cache** |
| Large dataset | > 1GB | 10-100 | **Parquet format** |
| Huge dataset | > 10GB | Any | **Parquet + shared cache** |
| Production pipeline | Any | Any | **Parquet format** |

---

## Implementation in cruijff_kit

### Current Implementation (2025-10-20)
The SLURM script generator (`tools/slurm/generate_slurm_scripts.py`) includes per-job caching by default:

```python
# In SLURM script template:
export HF_DATASETS_CACHE="/scratch/gpfs/$USER/.cache/hf-datasets-${SLURM_JOB_ID}"
mkdir -p "${HF_DATASETS_CACHE}"

echo "Using job-specific cache: ${HF_DATASETS_CACHE}"

# ... run training ...

# Cleanup
if [ -d "${HF_DATASETS_CACHE}" ]; then
    echo "Cleaning up job-specific cache: ${HF_DATASETS_CACHE}"
    rm -rf "${HF_DATASETS_CACHE}"
fi
```

### Customization
If you want to use a different caching strategy:

1. **Edit the SLURM script template** in `generate_slurm_scripts.py`
2. **Or manually edit generated scripts** before submission
3. **Or use a custom template** via `--template` flag

---

## Troubleshooting

### Cache-Related Errors

**Error: "Can't read json-train.arrow"**
- **Cause**: Cache corruption from concurrent writes
- **Solution**: Use per-job cache (already default)

**Error: "Permission denied" on cache directory**
- **Cause**: Cache created by different user or with wrong permissions
- **Solution**: Use per-job cache in your own scratch space

**Error: Out of disk space**
- **Cause**: Too many per-job caches accumulating
- **Solution**: Ensure cleanup step is working (check SLURM logs)

### Useful Commands

```bash
# Check cache size
du -sh ~/.cache/huggingface/datasets/

# Find all per-job caches
ls -lh /scratch/gpfs/$USER/.cache/hf-datasets-*/

# Clean up stale per-job caches (if cleanup failed)
rm -rf /scratch/gpfs/$USER/.cache/hf-datasets-*

# Check what dataset hash will be used
python -c "from datasets import load_dataset; print(load_dataset('json', data_files={'train': 'input/data.json'}, field='train').cache_files)"
```

---

## Best Practices Summary

1. ✅ **Use per-job cache by default** - It just works
2. ✅ **Convert to Parquet for production** - Better long-term solution
3. ✅ **Always include cleanup step** - Free up scratch space
4. ✅ **Monitor first few jobs** - Verify caching works as expected
5. ✅ **Document dataset formats** - Know which format you're using
6. ❌ **Don't assume shared cache is safe** - Race conditions can occur
7. ❌ **Don't skip overhead analysis** - Understand your storage/time costs

---

## References

**Related Issues**:
- [HuggingFace datasets #1234](https://github.com/huggingface/datasets/issues/1234) - Concurrent cache access
- [Experiment report](../cap_cross_eval_5_9_13L_2025-10-20/cache_collision_investigation.md) - Real-world cache collision investigation

**HuggingFace Docs**:
- [Cache management](https://huggingface.co/docs/datasets/cache)
- [Load from disk](https://huggingface.co/docs/datasets/loading)

**cruijff_kit Files**:
- [tools/slurm/generate_slurm_scripts.py](../tools/slurm/generate_slurm_scripts.py) - SLURM script generator
- [CLAUDE.md](../CLAUDE.md) - Project overview and conventions
